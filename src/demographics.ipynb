{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7458c8fc",
   "metadata": {},
   "source": [
    "# Learning about the Demograph of the data for the project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbfe5d3",
   "metadata": {},
   "source": [
    "The data for the project was taken from SensSmartTech Database on Physionet. \n",
    "It is polycardiograph of the cardiovascular signals measured synchronously. It consist of Electrocardiograph, phonocardiograph, photoplethysmography and accelerometer. It is consist of 338 30 seconds recordings from 32 healthy volunteers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b6789",
   "metadata": {},
   "source": [
    "It is made of 10 channels;\n",
    " 1. 4 ECG ( limb, V3, V4 leads)\n",
    " 2. 1 PCG (measured at the heart apex)\n",
    " 3. 4 PPG\n",
    " 4. 1 ACC ( accelerometer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81039e6f",
   "metadata": {},
   "source": [
    " Several multisensory databases capturing variations in HR during activity have been documented [5-8]. SensSmartTech stands out as the first base of multisensory recordings which systematically follows heart relaxation dynamics across a wide range of HRs (58 bpm - 173 bpm). The recorded HR-dependence is of interest to clinicians applying the HR biomarker correction, engineers investigating HR estimation by different wearable sensors and the impact of noises and artefacts on diagnostic signals, and scientist studying the underlying nonlinear dynamics of the heart as an electro-mechanical system.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d510be",
   "metadata": {},
   "source": [
    "Of interest to us is the  4 Channel ECG , 1 PCG and 1 ACC counting as 6 channels for the project work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a652ef54",
   "metadata": {},
   "source": [
    "### **Technicalities of the hardware used for taking the data.**\n",
    "\n",
    "1. ECG signal acquisition is performed with the ADS1298 chip (Texas Instruments) with the sampling rate set to 500 Hz. Measurement used 4 limb electrodes, V3 and V4, while the redundant precordial electrodes (V1, V2, V5 and V6) were placed on the upper right arm to prevent noise from the hanging leads\n",
    "    \n",
    "2. PCG signal is captured using a microphone ICS-40300 (TDK InvenSense) placed in a cardiology stethoscope SPIRIT CK-S474SPF63 (Spirit Medical) with the sampling rate of 1 kHz. 1 PCG stethoscope was positioned at the sternum to the right of V3 ECG electrode and secured with an elastic band\n",
    "    \n",
    "3. ACC signal was recorded by a MEMS accelerometer MPU6050 (TDK InvenSense) with an acceleration range set to +/- 1g. It was attached to the body between V3 and V4 ECG electrodes using a self-adhesive ECG electrode. Only the z axis in the direction perpendicular to the chest was used\n",
    "\n",
    " 4. Sensors output signals were digitalized by 16 bit A/D converters. The polycardiograph synchronously collected data form the sensors and transmitted them to a PC over Ethernet. Accuracy of the Polycardiograph was set by the sampling rate of the sensors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed41ba",
   "metadata": {},
   "source": [
    "**Relevant information concerning mode of data collection**: Recordings were taken in a standing position at rest and immediately after the activity.\n",
    "After each recording, the researcher calculated the heart rate (HR). Three 30-second recordings were made at rest. After the activity, recordings were repeated until the HR dropped to 10-20 bpm above the HR at rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464ea20c",
   "metadata": {},
   "source": [
    "Of interest to us is the CSV format which has column for time, and the rest of the channels associated with it.  The **acquisition time follows the sampling rate of the sensor**. Sensors may record signals at different point in time.\n",
    " Therefore, the time axes of different sensors are different, but the **acquisition is synchronized so that they can be extended to a common zero.**\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ce2c1d",
   "metadata": {},
   "source": [
    "Additionally, a table Demographics.csv lists file names and subject demographics, including age, height, weight, and body-mass index. Furthermore, each row in this table displays the subject activity status: 'B' for the measurement before and 'A' for the measurement after the activity, and the HR calculated as the inverse of the median RR interval per recording.\n",
    "To de-identify the data, all dates were removed from the recordings. The published data do not contain any information that identifies or provides a reasonable basis to identify an individual. The data comply with HIPPA requirements for sharing personal health information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99397b5",
   "metadata": {},
   "source": [
    "### **Understanding the Demographics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d223b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the necessary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from datetime import datetime, timedelta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Demographics_Cleaned.csv', )\n",
    "# Display the first few rows of the DataFrame to understand its structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09ffe21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the DataFrame\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values in each column:\\n\", missing_values)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d2898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Recording time' string to datetime.time object\n",
    "# Assuming 'Recording time (hh:mm:ss)' is the column with time strings\n",
    "\n",
    "# Function to safely convert time strings to datetime.time objects\n",
    "def parse_time(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(str(val).strip(), \"%H:%M:%S\").time()\n",
    "    except ValueError:\n",
    "        print(f\"Skipping invalid time format: {val}\")\n",
    "        return None\n",
    "\n",
    "# Apply the conversion\n",
    "df[\"Recording time\"] = df[\"Recording time (hh:mm:ss)\"].apply(parse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f1e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by Subject number and sort within each group by File number\n",
    "grouped = df.groupby(\"Subject number\")\n",
    "\n",
    "# Create a dictionary where key = Subject number, value = DataFrame of that subject's records\n",
    "subject_batches = {\n",
    "    subject: group.sort_values(by=\"File Number\").reset_index(drop=True)\n",
    "    for subject, group in grouped\n",
    "}\n",
    "\n",
    "# Display the first few rows of the DataFrame for a specific subject (e.g., Subject 1)\n",
    "subject_batches[1].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23e2600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the duration DataFrame\n",
    "# Drop rows with NaN in 'Recording time' to avoid issues in calculations\n",
    "\n",
    "df_clean = df.dropna(subset=[\"Recording time\"])\n",
    "\n",
    "# Group by Subject\n",
    "subject_groups = df_clean.groupby(\"Subject number\")\n",
    "\n",
    "# Prepare results\n",
    "duration_data = []\n",
    "\n",
    "for subject, group in subject_groups:\n",
    "    group_sorted = group.sort_values(\"File Number\")\n",
    "\n",
    "    start_time = group_sorted[\"Recording time\"].iloc[0]\n",
    "    end_time = group_sorted[\"Recording time\"].iloc[-1]\n",
    "\n",
    "    # Convert times to datetime so we can subtract\n",
    "    start_dt = datetime.combine(datetime.today(), start_time)\n",
    "    end_dt = datetime.combine(datetime.today(), end_time)\n",
    "\n",
    "    # Handle potential midnight wraparound (if needed)\n",
    "    if end_dt < start_dt:\n",
    "        end_dt += timedelta(days=1)\n",
    "\n",
    "    duration = end_dt - start_dt\n",
    "\n",
    "    duration_data.append({\n",
    "        \"Subject number\": subject,\n",
    "        \"Start time\": start_time,\n",
    "        \"End time\": end_time,\n",
    "        \"Duration (HH:MM:SS)\": duration\n",
    "    })\n",
    "\n",
    "# Create a new DataFrame with the results\n",
    "duration_df = pd.DataFrame(duration_data)\n",
    "duration_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb9dff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to group the data according to the before and after activity (A and B) and calculate the duration of each activity.\n",
    "\n",
    "# Clean the activity column (strip whitespace and uppercase)\n",
    "df[\"Activity\"] = df[\"Before (B)  / after (A) activity\"].astype(str).str.strip().str.upper()\n",
    "\n",
    "# Drop rows with missing info\n",
    "df_clean = df.dropna(subset=[\"Recording time\", \"Activity\"])\n",
    "\n",
    "# Prepare results\n",
    "results = []\n",
    "\n",
    "for subject, group in df_clean.groupby(\"Subject number\"):\n",
    "    subject_data = {\"Subject number\": subject}\n",
    "\n",
    "    for label in ['A', 'B']:  # A = After, B = Before\n",
    "        sub = group[group[\"Activity\"] == label]\n",
    "\n",
    "        if sub.empty:\n",
    "            subject_data[f\"{label} Start\"] = None\n",
    "            subject_data[f\"{label} End\"] = None\n",
    "            subject_data[f\"{label} Duration\"] = None\n",
    "            continue\n",
    "\n",
    "        sub_sorted = sub.sort_values(\"File Number\")\n",
    "\n",
    "        start_time = sub_sorted[\"Recording time\"].iloc[0]\n",
    "        end_time = sub_sorted[\"Recording time\"].iloc[-1]\n",
    "\n",
    "        start_dt = datetime.combine(datetime.today(), start_time)\n",
    "        end_dt = datetime.combine(datetime.today(), end_time)\n",
    "\n",
    "        if end_dt < start_dt:\n",
    "            end_dt += timedelta(days=1)\n",
    "\n",
    "        duration = end_dt - start_dt\n",
    "\n",
    "        subject_data[f\"{label} Start\"] = start_time\n",
    "        subject_data[f\"{label} End\"] = end_time\n",
    "        subject_data[f\"{label} Duration\"] = duration\n",
    "\n",
    "    # Optional: add total duration\n",
    "    if subject_data[\"A Duration\"] and subject_data[\"B Duration\"]:\n",
    "        subject_data[\"Total Duration\"] = subject_data[\"A Duration\"] + subject_data[\"B Duration\"]\n",
    "    else:\n",
    "        subject_data[\"Total Duration\"] = None\n",
    "\n",
    "    results.append(subject_data)\n",
    "\n",
    "# Create results DataFrame\n",
    "activity_duration_df = pd.DataFrame(results)\n",
    "activity_duration_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629c6502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the distribution of the each activity with a subject and finding the distribution of the activity finding for with\n",
    "# Heart Rate under each subject number.\n",
    "heart_rate_distr = df.groupby([\"Subject number\",\"Before (B)  / after (A) activity\"])[\"Median heart rate (bpm)\"].describe() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d274bda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_info = df[[\"Subject number\",\"Age (year)\",\"Height (cm)\",\"Weight (kg)\",\"Body-mass index\"]]\n",
    "\n",
    "# Drop duplicates so each subject appears only once\n",
    "unique_subject_info = subject_info.drop_duplicates(subset=[\"Subject number\"])\n",
    "\n",
    "# (Optional) Set subject as index\n",
    "unique_subject_info.set_index(\"Subject number\", inplace=True)\n",
    "unique_subject_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa7783",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = unique_subject_info.join(duration_df.set_index(\"Subject number\"), on=\"Subject number\", how=\"inner\")\n",
    "mean_series = heart_rate_distr[\"mean\"]\n",
    "mean_df = mean_series.unstack(\"Before (B)  / after (A) activity\")\n",
    "mean_df = mean_df.reset_index()\n",
    "mean_df.columns = ['Subject number', 'heart_rate_Before', 'heart_rate_After']\n",
    "new_df = new_df.join(mean_df.set_index(\"Subject number\"), on=\"Subject number\", how=\"left\")\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "88a8cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv(\"Final_Demographics.csv\", index=False)\n",
    "\n",
    "# I have had my fair share of challenges with the code, but I have tried to make it as clean as possible.\n",
    "# I have also tried to make the code as efficient as possible. I have used functions and loops where necessary to avoid redundancy.\n",
    "# I have also used pandas and numpy to handle the data efficiently. I have used groupby and apply functions to manipulate the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32872d62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ProJect64",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
